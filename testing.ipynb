{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code will extract text from the provided image containing handwritten notes using Tesseract OCR\n",
      "with the LSTM model for handwriting recognition. Adjustments to image preprocessing or Tesseract\n",
      "configuration options may be necessary depending on the quality and characteristics of your\n",
      "handwritten notes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_path = \"img.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Use Tesseract OCR to extract text\n",
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Parse the extracted text to differentiate between questions and answers\n",
    "# Your parsing logic goes here...\n",
    "\n",
    "# Example: Split text into lines and print\n",
    "lines = extracted_text.split(\"\\n\")\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code will extract text from the provided image containing handwritten notes using Tesseract OCR\n",
      "with the LSTM model for handwriting recognition. Adjustments to image preprocessing or Tesseract\n",
      "configuration options may be necessary depending on the quality and characteristics of your\n",
      "handwritten notes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_path = \"img.png\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Use Tesseract OCR with LSTM model for handwriting recognition\n",
    "# Additional options can be passed using config parameter, e.g., '--psm 6' for page segmentation mode\n",
    "extracted_text = pytesseract.image_to_string(gray_image, lang=\"eng+handwriting\")\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from hwrt.handwritten import handwritten\n",
    "\n",
    "# # Path to the image containing handwritten text\n",
    "# image_path = \"handwritten_note.jpg\"\n",
    "\n",
    "# # Read the image using OpenCV\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Initialize the handwritten text recognition model\n",
    "# model = handwritten.HandwrittenData()\n",
    "\n",
    "# # Recognize handwritten text in the image\n",
    "# result = model.recognize(image)\n",
    "\n",
    "# # Extract the recognized text\n",
    "# recognized_text = result[\"text\"]\n",
    "\n",
    "# # Print the recognized text\n",
    "# print(recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import vision\n",
    "# import io\n",
    "\n",
    "# # Path to the image containing handwritten text\n",
    "# image_path = \"notes1.png\"\n",
    "\n",
    "# # Instantiates a client\n",
    "# client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# # Read the image file\n",
    "# with io.open(image_path, \"rb\") as image_file:\n",
    "#     content = image_file.read()\n",
    "\n",
    "# # Construct an image instance\n",
    "# image = vision.Image(content=content)\n",
    "\n",
    "# # Perform text detection\n",
    "# response = client.document_text_detection(image=image)\n",
    "# document = response.full_text_annotation\n",
    "\n",
    "# # Extract the recognized text\n",
    "# recognized_text = document.text\n",
    "\n",
    "# # Print the extracted text\n",
    "# print(recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load the pre-trained CRNN model\n",
    "# # Replace 'model_path' with the path to your pre-trained model file\n",
    "# model_path = 'pretrained_crnn_model.pth'\n",
    "# model = torch.load(model_path)\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Define transformations to preprocess images for the model\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 100)),  # Resize image to the expected input size of the model\n",
    "#     transforms.ToTensor(),          # Convert image to tensor\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize image pixel values\n",
    "# ])\n",
    "\n",
    "# # Function to perform handwritten text recognition\n",
    "# def recognize_handwritten_text(image_path, model):\n",
    "#     # Read the image using OpenCV\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     # Convert the image to grayscale\n",
    "#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Apply thresholding to enhance the handwritten text\n",
    "#     _, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "#     # Convert OpenCV image (numpy array) to PIL Image\n",
    "#     pil_image = Image.fromarray(threshold_image)\n",
    "\n",
    "#     # Apply transformations to preprocess the image for the model\n",
    "#     input_image = transform(pil_image)\n",
    "\n",
    "#     # Add batch dimension and convert to float tensor\n",
    "#     input_image = input_image.unsqueeze(0).float()\n",
    "\n",
    "#     # Perform inference\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_image)\n",
    "\n",
    "#     # Decode the predicted output\n",
    "#     # (You may need to implement a decoding function based on the output format of your model)\n",
    "#     predicted_text = decode_output(output)\n",
    "\n",
    "#     return predicted_text\n",
    "\n",
    "# # Example of decoding function (replace with your actual decoding logic)\n",
    "# def decode_output(output):\n",
    "#     # Convert output tensor to numpy array\n",
    "#     output_array = output.numpy()\n",
    "\n",
    "#     # Convert numpy array to text (replace with your actual decoding logic)\n",
    "#     # For example, you might use a dictionary to map output indices to characters\n",
    "#     # Then, concatenate the characters to form the predicted text\n",
    "#     predicted_text = ''\n",
    "#     for idx in output_array:\n",
    "#         predicted_text += chr(idx)\n",
    "\n",
    "#     return predicted_text\n",
    "\n",
    "# # Path to the image containing handwritten text\n",
    "# image_path = 'handwritten_note.jpg'\n",
    "\n",
    "# # Recognize handwritten text in the image\n",
    "# predicted_text = recognize_handwritten_text(image_path, model)\n",
    "\n",
    "# # Print the predicted text\n",
    "# print(\"Predicted Text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1:\n",
      "Question 1.\n",
      "What is the kind of pain and ache that the poet feels ?\n",
      "\n",
      "Answer 1:\n",
      "Answer:\n",
      "The poet (here poetess) is deeply attached to her mother who is pretty aged, weak and pale. She is troubled\n",
      "to think that the old mom might depart in her absence\n",
      "\n",
      "Question 2:\n",
      "Question 2.\n",
      "Why are the young trees described as ‘sprinting’ ?\n",
      "\n",
      "Answer 2:\n",
      "Answer:\n",
      "The young trees running spiritedly stand in sharp contrast to the aged and pale looking mother. The trees\n",
      "symbolise youth and life, whereas the old mother is slipping towards the grave.\n",
      "\n",
      "Question 3:\n",
      "Question 3.\n",
      "Why has the poet brought in the image of the merry children spilling out of their homes ?\n",
      "\n",
      "Answer 3:\n",
      "Answer:\n",
      "The little children are full of life, hope and cheerfulness. They have just begun life and have a long way to go.\n",
      "The old and weak mother of the poetess, however, is fast losing hold on life. She could breath her last any\n",
      "day in near future. The image of cheerful children makes the sight of the mother all the more painful.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Path to the image containing both questions and answers\n",
    "image_path = \"multiqna.png\"\n",
    "\n",
    "# Read the image using OpenCV\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding to enhance the text\n",
    "_, threshold_image = cv2.threshold(\n",
    "    gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU\n",
    ")\n",
    "\n",
    "# Use Tesseract OCR to extract text from the image\n",
    "extracted_text = pytesseract.image_to_string(threshold_image)\n",
    "\n",
    "# Split the extracted text into lines\n",
    "lines = extracted_text.split(\"\\n\")\n",
    "\n",
    "# Define variables to store questions and answers\n",
    "questions = []\n",
    "answers = []\n",
    "current_block = []\n",
    "\n",
    "\n",
    "# Function to process the current block and determine if it's a question or an answer\n",
    "def process_block(current_block):\n",
    "    block_text = \"\\n\".join(current_block)\n",
    "    if any(keyword.lower() in block_text.lower() for keyword in [\"question\", \"q:\"]):\n",
    "        questions.append(block_text)\n",
    "    elif any(\n",
    "        keyword.lower() in block_text.lower() for keyword in [\"ans\", \"answer:\", \"a:\"]\n",
    "    ):\n",
    "        answers.append(block_text)\n",
    "\n",
    "\n",
    "# Loop through each line of text and process question and answer blocks\n",
    "for line in lines:\n",
    "    # Strip leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # Skip empty lines\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    # Check if the line indicates the start of a new block\n",
    "    if any(\n",
    "        keyword.lower() in line.lower()\n",
    "        for keyword in [\"question\", \"q:\", \"ans\", \"answer:\", \"a:\"]\n",
    "    ):\n",
    "        # Process the current block and reset it\n",
    "        if current_block:\n",
    "            process_block(current_block)\n",
    "            current_block = []\n",
    "\n",
    "    # Add the line to the current block\n",
    "    current_block.append(line)\n",
    "\n",
    "# Process the last block\n",
    "if current_block:\n",
    "    process_block(current_block)\n",
    "\n",
    "# Print all questions and answers\n",
    "for i, (question, answer) in enumerate(zip(questions, answers), 1):\n",
    "    print(f\"Question {i}:\\n{question}\\n\")\n",
    "    print(f\"Answer {i}:\\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (c:\\Users\\dev07\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'OpenAI' from 'openai' (c:\\Users\\dev07\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = \"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\"\n",
    "# openai.api_key =\n",
    "client = OpenAI(api_key=api_key)\n",
    "# Example answer key provided by the teacher\n",
    "question = \"What is the kind of pain and ache that the poet feels ?\"\n",
    "teacher_answer = \"\"\"The poet (here poetess) is deeply attached to her mother who is pretty aged, weak and pale. She is troubled\n",
    "to think that the old mom might depart in her absence\"\"\"\n",
    "# Example answer provided by the student\n",
    "student_answer = \"\"\"The poet  is deeply attached to her mother who is very aged and weak. She is troubled\n",
    "to think that the old mom might depart in her absence\"\"\"\n",
    "\n",
    "# Concatenate the teacher's answer and the student's answer\n",
    "prompt = f\"Question: {question.lower()}\\nCorrect Answer: {teacher_answer.lower()}\\nStudent's Answer: {student_answer.lower()}\\n give marks to student's answer out of 10(just number):\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model=\"davinci-002\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=3,\n",
    "    stop=[\"\\n\"],\n",
    "    temperature=0,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "# response\n",
    "completion_text = response.choices[0].text.strip()\n",
    "\n",
    "# # Print the similarity score\n",
    "print(\"Similarity Score:\", completion_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_Topic = \"\"\" You are a assistant that is checking and grading the answer sheet of students by comaring it with the answers provided by the teacher which is the answer key. \\n You give to the point accurate marks to the students from 0.00 to 1.00 in the format of 'Question no.' : 'Score'.\n",
    "So get in the form of a numbered list of Questions and Score as specified in the format above.\"\"\"\n",
    "\n",
    "prompt = \"\"\" \n",
    "I will provide you the Answer key first marked as 'Answer key' and then the answers of student as 'Student answer:' which will contains a list of 'Question' and its corresponding 'Answer'.\n",
    "I want you to give marks between 0.00 to 1.00 to each answer. Also compare the meaning of both the answers and than give the score. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# kw_model = KeyBERT()\n",
    "# keywords = kw_model.extract_keywords(teacher_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Keyword Extraction using KeyBERT\n",
    "# model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\n",
    "# teacher_keywords = model.extract_keywords(teacher_answer)\n",
    "# student_keywords = model.extract_keywords(student_answer)\n",
    "\n",
    "# # 2. Keyword Similarity using Cosine Similarity with TF-IDF vectors\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# keywords_corpus = [\" \".join(teacher_keywords), \" \".join(student_keywords)]\n",
    "# tfidf_matrix = vectorizer.fit_transform(keywords_corpus)\n",
    "# keyword_similarity = cosine_similarity(tfidf_matrix)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testinput1 = \"\"\" Answer Key:\\n\\nQuestion 1: What is Object-Oriented Programming (OOP)?\\nAnswer1: Object-Oriented Programming (OOP) is a programming paradigm that organizes code into objects, which are instances of classes. It emphasizes the concept of \"objects\" that encapsulate data and behavior. OOP principles include encapsulation, inheritance, and polymorphism, providing a modular and organized approach to software development.\\nQuestion 2: Explain Encapsulation in OOP.\\nAnswer2: Encapsulation is the OOP principle that involves bundling data (attributes) and methods (functions) that operate on the data into a single unit known as an object. It helps in hiding the internal details of an object and restricting direct access to its implementation. Encapsulation enhances code organization, reduces complexity, and promotes data integrity by controlling access to the internal state of objects.\\nQuestion 3: What is Inheritance in OOP?\\nAnswer3: Inheritance is a fundamental OOP concept that allows a new class (subclass or derived class) to inherit attributes and behaviors from an existing class (superclass or base class). This promotes code reusability and establishes a hierarchy of classes. The subclass can extend or override the functionality of the superclass while inheriting its characteristics.\\nQuestion 4: Describe the concept of Polymorphism in OOP.\\nAnswer4: Polymorphism in OOP refers to the ability of objects to take on multiple forms or the ability of a method to perform different actions based on the object it is acting upon. This can be achieved through method overloading (same method name, different parameters) and method overriding (same method name and parameters, different implementation). Polymorphism enhances flexibility and adaptability in code design.\\nQuestion 5: What is Abstraction in the context of Object-Oriented Programming?\\nAnswer5: Abstraction is the process of simplifying complex systems by modeling classes based on the essential properties and behaviors relevant to the problem at hand, while ignoring unnecessary details. It involves creating abstract classes or interfaces that define a common structure without specifying the implementation. Abstraction helps in managing complexity, improving code readability, and facilitating code maintenance.\\nAnswer Sheet:\\n\\nAnswer1: Object-Oriented Programming (OOP) is a programming paradigm that utilizes the concept of objects to structure code. Objects encapsulate data and behavior, allowing for a more organized and modular approach to software development. OOP principles, such as encapsulation, inheritance, and polymorphism, facilitate code reuse, maintainability, and the modeling of real-world entities.\\nAnswer2: Encapsulation is when you put your data and functions in a box (object) so that they stay together. It's like keeping your stuff in a container, so you don't mess things up.\\nAnswer3: Inheritance is like when a new class can borrow things from an old class. It's kinda like passing down traits or features from a parent to a child. Makes it easier to reuse code and saves time.\\nAnswer4: Polymorphism is when you can do different things with the same method. It's like having one remote control that works for different devices, each button doing a unique thing. It makes code more flexible.\\nAnswer5: Abstraction is like hiding the complicated parts and just showing the simple stuff. It's like using a TV remote without knowing all the technical details. Makes it easier to understand and use.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testout1 = \"\"\"1: 1.00\\n2: 0.64\\n3: 0.82\\n4: 0.75\\n5: 0.80\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testinput2 = \"\"\" \n",
    "Answer Key:\n",
    "Question 1:\n",
    "What is the kind of pain and ache that the poet feels ?\n",
    "Answer 1:\n",
    "The poet (here poetess) is deeply attached to her mother who is pretty aged, weak and pale. She is troubled\n",
    "to think that the old mom might depart in her absence\n",
    "\n",
    "Question 2:\n",
    "Why are the young trees described as 'sprinting' ?\n",
    "Answer 2:\n",
    "The young trees running spiritedly stand in sharp contrast to the aged and pale looking mother. The trees\n",
    "symbolise youth and life, whereas the old mother is slipping towards the grave.\n",
    "\n",
    "Question 3:\n",
    "Why has the poet brought in the image of the merry children spilling out of their homes ?\n",
    "Answer 3:\n",
    "The little children are full of life, hope and cheerfulness. They have just begun life and have a long way to go.\n",
    "The old and weak mother of the poetess, however, is fast losing hold on life. She could breath her last any\n",
    "day in near future. The image of cheerful children makes the sight of the mother all the more painful.\n",
    "\n",
    "Answer Sheet:\n",
    "\n",
    "Ans1: poet is worried that her mother may die soon, hence she is in pain.\n",
    "Ans2: that is because to symbolise youth and life as he mom is dieing.\n",
    "Ans3: poet brought the image as she was feeling pain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testout2 = \"\"\"1: 0.48\\n2: 0.50\\n3: 0.12\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.txt\", \"r\") as file:\n",
    "    papercheck = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: usecase_Topic},\n\u001b[0;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt \u001b[38;5;241m+\u001b[39m testinput1},\n\u001b[0;32m      6\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: testout1},\n\u001b[0;32m      7\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt \u001b[38;5;241m+\u001b[39m testinput2},\n\u001b[0;32m      8\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: testout2},\n\u001b[0;32m      9\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt \u001b[38;5;241m+\u001b[39m papercheck},\n\u001b[0;32m     10\u001b[0m     ],\n\u001b[0;32m     11\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     12\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     14\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": usecase_Topic},\n",
    "        {\"role\": \"user\", \"content\": prompt + testinput1},\n",
    "        {\"role\": \"assistant\", \"content\": testout1},\n",
    "        {\"role\": \"user\", \"content\": prompt + testinput2},\n",
    "        {\"role\": \"assistant\", \"content\": testout2},\n",
    "        {\"role\": \"user\", \"content\": prompt + papercheck},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.2,\n",
    "    presence_penalty=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.30\n",
      "2: 0.55\n",
      "3: 0.80\n",
      "4: 0.75\n",
      "5: 0.70\n"
     ]
    }
   ],
   "source": [
    "result = response.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
